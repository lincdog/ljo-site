{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home Welcome . My name is Lincoln Ombelets , and I'm currently a graduate student in the Cai Lab at Caltech . Posts","title":"Home"},{"location":"#home","text":"Welcome . My name is Lincoln Ombelets , and I'm currently a graduate student in the Cai Lab at Caltech .","title":"Home"},{"location":"#posts","text":"","title":"Posts"},{"location":"about/","text":"Background and education My name is Lincoln Ombelets. I grew up in the Boston area and attended Northeastern University in Boston. Growing up, I learned programming and a bit of software development, but in college I studied chemistry. Now I'm a third-year graduate student in the chemistry PhD program at Caltech in Pasadena, California, where my dual interests of coding and chemistry have each found use. I work in the lab of Long Cai in the Biology and Biological Engineering Division. In general, we are interested in using fluorescence microscopes to study biological systems at a very high resolution. The Cai Lab developed a technique known as seqFISH , which allows researchers to ascertain the position and abundance of many thousands of different biomolecules in the same sample . This contrasts with traditional imaging methods which are generally limited to visualizing fewer than ten distinct molecules in the same sample. In 2020, I transitioned from experimental technique development to computational tool development. The data produced by experiments run in our lab is tremendously rich and complex, being three-dimensional spatial maps of up to thousands of different biomolecules in several different classes. Motivated by the conceptual and computational difficulties of extracting insight from these data, I am developing a data wrangling and 3D visualization platform. I hope to facilitate the exploration, processing and interpretation of the data produced by spatial 'omics experiments so that biologists can more fully leverage their tremendous potential to find new insights. The challenge of biological complexity Although I studied chemistry in college, and conducted research synthesizing new drug-like molecules, I was drawn to biological research when I learned about the incredible complexity and coordination found in living systems. Biological complexity is fundamental chemical in nature, and unlike humans, even the simplest organisms are able to manipulate highly complex molecules with astonishing precision. Furthermore, the scale of living complexity covers many orders of magnitude - certainly from individual atoms or molecules to many-meter-scale organisms like trees or whales, and potentially to multi-individual collective systems depending on one's definitions. Understanding the principles behind this complexity, and how it adapts to challenges, is a great undertaking for humanity. How can we grapple with the nearly infinite amount of information and work it seems it would require to understand such systems? The challenge of society Another great challenge for humanity is learning how to build societies that are fairer, kinder and more equitable than previous generations. During the coronavirus pandemic and the George Floyd protests, I have started reflecting more on sociopolitical and philosophical issues than ever before. What is the best way to govern large numbers of people? What is the role of the media? What is the role of the economic system? How can we educate future generations so that the mistakes of the past and present are not endlessly repeated? How can I even conceive of these issues which, like biology, span many scales of complexity and involve unknown effects of unknown magnitudes? I am trying to become better informed and better read on some of the philosophers and thinkers that have historically pondered these big topics. These are some of the overarching questions which I am curious about - philosophy of science, interdisciplinary research, and political/economic philosophy. I am really at the very beginning of learning about most of these things, and I have never blogged or posted much on social media. But I hope to use this website to air some of my nascent and evolving thoughts on these issues. I welcome constructive feedback via email .","title":"About"},{"location":"about/#background-and-education","text":"My name is Lincoln Ombelets. I grew up in the Boston area and attended Northeastern University in Boston. Growing up, I learned programming and a bit of software development, but in college I studied chemistry. Now I'm a third-year graduate student in the chemistry PhD program at Caltech in Pasadena, California, where my dual interests of coding and chemistry have each found use. I work in the lab of Long Cai in the Biology and Biological Engineering Division. In general, we are interested in using fluorescence microscopes to study biological systems at a very high resolution. The Cai Lab developed a technique known as seqFISH , which allows researchers to ascertain the position and abundance of many thousands of different biomolecules in the same sample . This contrasts with traditional imaging methods which are generally limited to visualizing fewer than ten distinct molecules in the same sample. In 2020, I transitioned from experimental technique development to computational tool development. The data produced by experiments run in our lab is tremendously rich and complex, being three-dimensional spatial maps of up to thousands of different biomolecules in several different classes. Motivated by the conceptual and computational difficulties of extracting insight from these data, I am developing a data wrangling and 3D visualization platform. I hope to facilitate the exploration, processing and interpretation of the data produced by spatial 'omics experiments so that biologists can more fully leverage their tremendous potential to find new insights.","title":"Background and education"},{"location":"about/#the-challenge-of-biological-complexity","text":"Although I studied chemistry in college, and conducted research synthesizing new drug-like molecules, I was drawn to biological research when I learned about the incredible complexity and coordination found in living systems. Biological complexity is fundamental chemical in nature, and unlike humans, even the simplest organisms are able to manipulate highly complex molecules with astonishing precision. Furthermore, the scale of living complexity covers many orders of magnitude - certainly from individual atoms or molecules to many-meter-scale organisms like trees or whales, and potentially to multi-individual collective systems depending on one's definitions. Understanding the principles behind this complexity, and how it adapts to challenges, is a great undertaking for humanity. How can we grapple with the nearly infinite amount of information and work it seems it would require to understand such systems?","title":"The challenge of biological complexity"},{"location":"about/#the-challenge-of-society","text":"Another great challenge for humanity is learning how to build societies that are fairer, kinder and more equitable than previous generations. During the coronavirus pandemic and the George Floyd protests, I have started reflecting more on sociopolitical and philosophical issues than ever before. What is the best way to govern large numbers of people? What is the role of the media? What is the role of the economic system? How can we educate future generations so that the mistakes of the past and present are not endlessly repeated? How can I even conceive of these issues which, like biology, span many scales of complexity and involve unknown effects of unknown magnitudes? I am trying to become better informed and better read on some of the philosophers and thinkers that have historically pondered these big topics. These are some of the overarching questions which I am curious about - philosophy of science, interdisciplinary research, and political/economic philosophy. I am really at the very beginning of learning about most of these things, and I have never blogged or posted much on social media. But I hope to use this website to air some of my nascent and evolving thoughts on these issues. I welcome constructive feedback via email .","title":"The challenge of society"},{"location":"publications/","text":"Pre-Caltech 2020 Zeng, M., Xiong Y., Safaee, N., ... Ombelets, L., ... Westover, K., Fischer, E., Gray, N. 2020. Exploring Targeted Degradation Strategy for Oncogenic KRAS G12C . Cell Chem. Biol. 27, 19-31. 2019 Moulton, K., Sadiki, A., Koleva, B.N., Ombelets, L.J. , ... Zhou, Z.S. 2019. Site-Specific Reversible Protein and Peptide Modification: Transglutaminase-Catalyzed Glutamine Conjugation and Bioorthogonal Light-Mediated Removal . Bioconj. Chem. 30, 1617-1621.","title":"Publications"},{"location":"publications/#pre-caltech","text":"","title":"Pre-Caltech"},{"location":"publications/#2020","text":"Zeng, M., Xiong Y., Safaee, N., ... Ombelets, L., ... Westover, K., Fischer, E., Gray, N. 2020. Exploring Targeted Degradation Strategy for Oncogenic KRAS G12C . Cell Chem. Biol. 27, 19-31.","title":"2020"},{"location":"publications/#2019","text":"Moulton, K., Sadiki, A., Koleva, B.N., Ombelets, L.J. , ... Zhou, Z.S. 2019. Site-Specific Reversible Protein and Peptide Modification: Transglutaminase-Catalyzed Glutamine Conjugation and Bioorthogonal Light-Mediated Removal . Bioconj. Chem. 30, 1617-1621.","title":"2019"},{"location":"reading/","text":"Updated 12/30/2020 Currently Reading The Tyranny of Merit by Michael Sandel Elementary Applied Topology by Robert Ghrist In Search of Lost Time by Marcel Proust Thich Nhat Hanh: Essential Writings On Deck The Tacit Dimension by Michael Polanyi We Have Never Been Modern by Bruno Latour Laboratory Life by Bruno Latour and Steve Woolgar Against Method by Paul Feyerabend Envisioning Information by Edward Tufte","title":"Reading List"},{"location":"reading/#currently-reading","text":"The Tyranny of Merit by Michael Sandel Elementary Applied Topology by Robert Ghrist In Search of Lost Time by Marcel Proust Thich Nhat Hanh: Essential Writings","title":"Currently Reading"},{"location":"reading/#on-deck","text":"The Tacit Dimension by Michael Polanyi We Have Never Been Modern by Bruno Latour Laboratory Life by Bruno Latour and Steve Woolgar Against Method by Paul Feyerabend Envisioning Information by Edward Tufte","title":"On Deck"},{"location":"posts/2020/bio-future/","text":"Highlights In highly complex systems such as computers or living things, * complexity exists at many scales, and it is nontrivial to figure out appropriate coarse-grained or abstracted descriptions of the system's operation * high levels of variability may exist on multiple scales, but certain features are conserved . Knowing the difference between relevant and superfluous variability for a given question allows dimensionality reduction and is essential for reducing the difficulty of understanding these systems * \"understanding\" can be defined in many ways, and the level of abstraction allowed will depend on the specific questions or problems at hand. Knowing the most effective \"levers\" to manipulate the system at multiple scales could be a general goal of efforts to solve problems in a given system. Biology's complexity problem Biological research is still in its infancy in terms of performing precisely controlled experiments, as only around 40 years ago we did not even have the tools to perturb the informational language of cells - recombinant DNA and genetic engineering. Now that these exist and are rapidly improving, biology has entered an exciting age of discovery and advancement. However, we still largely lack the tools to precisely manipulate small numbers of cells in complex ways, or to make highly precise measurements on many parameters simultaneously. In the first case, desired or unexpected outcomes may have been missed because the \"right\" perturbation may not be simple. In the second case, an interesting outcome could have been missed because the \"right\" parameters (possibly thousands) were not measured. In my opinion, biology needs to be able to induce highly complex perturbations and measure simple outcomes, or induce simple perturbations and measure highly complex outcomes. Right now, we are mostly inducing simple perturbations and measuring simple outcomes. Aliens discover a Dell I think the following thought experiment demonstrates well the difficulties of studying biological complexity. There is no reason to suspect our brains evolved to introspect into their own inner workings as we have been attempting. To model a similar scenario, imagine that a race of aliens visits earth and discovers our personal computers. Though they're intelligent, they have much different physiological and sensory parameters than us, and their equivalent of a computer is nothing like ours. Perhaps they interact with it not through touch and sight, but through electromagnetic radiation they emit, or precise mechanical vibrations. They don't necessarily use written language and certainly don't understand ours. Anyway, they decide to try to perform experiments on some earthly computers they found to try and understand what they do and how they work. How would this turn out? Because they have a very different way of interacting with their environment, their intuition will be mostly wrong. They will probably try first to interact with our computer as they do with theirs. This could lead to breaking a bunch of test objects, as their mechanical force or radiation or whatever is not compatible with the construction of our computers. Perhaps eventually they find the right scale of force to apply without breaking stuff. Assuming they have a properly-connected keyboard and mouse, maybe they try pressing many combinations of buttons at different times. Eventually they are lucky enough to figure out that pushing one particular button - the power button - results in a dramatic response: the display starts emitting light in complicated, time-dependent patterns! This would be an alien Nobel-worthy achievement. They now can seek to understand how the inputs correspond with the light patterns on the display, and how this is all coordinated under the hood. However, the space of possible inputs is gigantic - will they try every combination of the many keyboard buttons across many time points before or after the pressing of the power button? That will take a tremendous amount of time. We humans know that almost all of their attempts will do nothing meaningful. However, suppose they eventually somehow log in to the desktop. What a breakthrough! But they will find much a higher level of complexity in the on-screen patterns, and moreover that the patterns seem to change stochastically. What if the computer needs to update? What if Chrome opens on startup? It would be very easy to erroneously attribute these 'random' changes to the detailed sequence of perturbations (i.e. button presses, etc) that the experimenters are using, because they do not know the underlying principles. If the experimenters understand electricity, maybe they can grasp from their dissection efforts that the innards of the computer conduct electric current in highly complex and precise ways. They could then take a more bottom-up approach by trying to figure out which channels and components are important for the apparent function of the computer. This too is an enormous problem space - there are literally billions of transistors in a modern CPU alone. Of course, what readout they use of 'function' is very important: if all they have is whether the display lights up when the power button is pressed, they will not gain much information about how the component in question actually contributes to the computer's function. Let's say the experimenters had a few different types of computers - Windows and Mac, laptops and desktops, maybe even smartphones. How will they know the commonalities? From zero knowledge, different types of computers look very distinct. It should become clear that they all have a display, and some keyboard interface to input information: these are conserved features which can hint at their importance. However, let's also say they have two different models of Windows PCs - a Dell laptop and a Lenovo laptop, for example. While they look mostly similar, the layout and dimensions of the keyboards are noticeably different, there are some indicator lights and buttons that are different between them, and their on-screen light patterns differ somewhat. How will the experimenters determine if these differences are important or not? This illustrates the challenge of identifying significant variability . The significance of a given type of variability always depends on one's purpose - for certain questions, it matters a lot whether you have a Mac or PC, but for other more \"conserved\" features it doesn't. If you want to browse the web, the hardware of the computer doesn't really matter; if you want to perform complex climate modeling, it matters a great deal. Discussion The above few examples illustrate some basic absurdities and problems encountered when studying a complex multiscale system even in relatively productive lines of inquiry - knowing that the computer works by orchestrating the flow of electric current, and figuring out how to power it on and use the keyboard for inputs are correct tracks. Similarly, we have made great progress with identifying conserved low-level elements of biology in the form of the genetic code, the central dogma, and molecular evolution. Most people with a decent grasp on coding and basic hardware would probably say they \"know how a computer works\", yet they do not know or understand the complete circuit diagram of any part of the machine. But these tools are the levers which most effectively manipulate computers at most scales in a general way. A complete reductionist understanding of the atomistic parts isn't necessary because we have added layers of abstraction on top of it that allow us to effectively manipulate the computer's function without directly dealing with this complexity. The closest one can get to the circuits on most computers is assembly language, which still doesn't deal directly with transistors and logic gates. In a system as complex as a modern computer, it would simply require too much information to deal with these details. Similarly, changes to these low-level details tend not to affect the higher-level behavior in very noticeable ways because they are designed to smooth over and adapt to many different low-level configurations. Changes at all levels seem to become more important when the system is pushed to a limit, though: when running extremely intensive computations, the CPU architecture, assembly language, cache sizes and other low-level features start to matter a lot. Perhaps similarly, the function of many genes or other biological components becomes most apparent in stressed or diseased organisms. Layers of abstraction also allow us to disregard certain levels of variability. While the circuit diagrams of two different brands of motherboards or CPUs may differ wildly, only the bottom-most levels of the system interact with them, and they present a consistent coarse-grained picture to the upper levels so that the overall operation of the system is mostly unchanged. Abstraction and coarse-graining allow underspecification of low-level details, like how macroscopic thermodynamic observables describe a many-component system without specifying the position and momentum of each component. I think a major challenge of contemporary biology is determining which variability is important for a given question and which can be coarse-grained out. With increasing ability to generate huge, high-dimensional datasets, we always observe variability at many levels. Without further insights into which quantities or characteristics are conserved or important for given questions, we are stuck in a data-rich, understanding-poor regime. We are also prone to erroneously attribute causal power to coincidental correlations. Think of the aliens recording the shape and size of every key on a Mac versus a PC and positing that these are causally relevant to the differences between the two. Fixing most problems in a computer also doesn't require detailed knowledge of the low-level details. The command line and GUI present the levels for manipulating things at many levels. Hardware problems are usually handled by modularity - instead of tracking down the exact circuit that is causing a GPU failure, the entire unit can be removed and replaced without understanding any of the details. Medicine does a similar thing with organ transplants - we don't know how to grow a new heart yet or exactly understand how it works at every level, but we have identified empirical \"boundaries\" that allow it and other organs to be treated (imperfectly) as modules. Similarly in software, portions of the RAM content of a modern computer can be examined in relative isolation as the operating system logically allocates them to processes. I wonder if understanding \"modules\" at many levels and the levers that manipulate them will be a fruitful approach for human diseases. Traditional pharmaceutical approaches tend to target low-level elements such as specific proteins that are thought to be compromised in a disease. These approaches lie on the assumption that molecular targets are an appropriate module and lever for modulating the system's behavior. They work best when this assumption is most true - digestive enzyme deficienies and cystic fibrosis come to mind. But they tend to fail or have mixed results with conditions that are multiscale or involve high order interactions, like mental health conditions. Imagine trying to fix a problem in Microsoft Word by inactivating a certain capacitor on the motherboard that interacts with Word. It might sort of fix (at least the symptoms) of the problem, but many other programs depend on that component, and Word is affected by many subsystems of the computer beyond that component. High-level interventions are showing promise for certain extremely complex conditions, especially neurodegenerative and mental health conditions for which typical reductionist approaches fall short. The major challenge is that unlike computers, biology was not designed by humans and had no reason to designate scales, modules, and abstraction levels according to any scheme that is intuitive or sensical to our brains. It verges on an epistemological question whether we can ever fully grasp these partitions, if they exist. It seems to me that using mathematical principles to remove our flawed intuition and biases is important (however, mathematics may be a construct reflecting the structure of the human mind as well; perhaps a future post on this). Conclusion: where from here? At the beginning I mentioned the scale of perturbations and measurements we're able to make. It would seem one way the aliens could proceed is by high-dimensional perturbations followed by simple readouts, such as whether the computer turns on. Another way is simple perturbations followed by high-dimensional readouts, such as contents of the display, voltage recordings on various components, etc. Both of these options are not great. They both involve 'brute-force' explorations of enormous spaces of possible perturbations or measurements. Because we designed computers to make sense, we know exactly which perturbations and measurements one should make. But with our own biology, we have only beginnings of ideas of what scales and levers are most relevant to perturb and measure. I tend to prefer the one perturbation-many measurements regime, which is what I work on in my PhD and what a lot of modern biology focuses on, but I'm not sure it's objectively superior. Both spaces, of perturbations and measurements, are basically infinite a priori , but the space of practical measurements is limited/defined by the current experimental methods. This makes it seem more tractable to try to understand effects of certain simple perturbations at many scales as measurement methods improve. Plus it seems easier to associate cause with a simple perturbation than with many, where further dissection would be required to identify necessary and accessory elements. Both perturbation and measurement techniques encounter difficulties applying across scales, for example to in vitro cultured cells then to whole organisms. Measuring low-level parameters such as individual genes in individual cells is possible in vitro and in small organismal systems such as parts of mouse organs or embryos; but it becomes virtually intractable at the scale of whole mouse (let alone human) organs, since these datasets scale with the volume of the system. The neuroscience subfield of connectomics encounters dramatically these scaling problems. This field attempts to produce a connection map of all the synapses of an organism's nervous system. So far, only the ~6000 connections in the millimeter-scale nematode Caenorhabditis elegans have been fully mapped. Heroic efforts are currently underway to map the millions of connections in the fruitfly, producing an enormous amount of data. Scaling up to mouse and eventually human seems impractical at the current rate of experiments, and the capacity of data storage and analysis: the human brain has on the order of $10^{15}$ synapses. Furthermore, this \"circuit diagram\" approach would seem to encounter a big problem of inter-individual variability. C. elegans have a prototyped nervous system that forms about the same connections each time - each of its few hundred neurons has its own name and neighbors. More complex organisms will not provide such a luxury. Once the incredible effort to map the fruitfly or rodent connectome is complete, the question will remain: what is the significance of each connection? How prototyped are they? The C. elegans connectome paved the way for much smarter questioning and is an invaluable reference, but it by no means \"solved\" the animal's neuroscience. Similarly, knowing the incredibly complex circuit diagram of my Dell laptop will not mean the aliens have finished their task and understand human-made computers. Moreover, this knowledge will hardly transfer to a different brand and architecture of computer, even though we know that these share many abstract similarities. Some knowledge of the conserved features that provide for abstraction and coarse-graining is necessary. Overall, we probably need to take measurements at multiple scales, and consider how layers of abstraction might be built in to biology for robustness and efficiency. What \"interfaces\" do low-level components provide for larger-scale manipulation? What assumptions do higher-level modules make about lower-level ones? I think we have enough knowledge to make some educated first guesses about some of this. I'm not sure if we have the measurement tools yet to directly query them. That is perhaps why this type of complex systems thinking applied to biology has mostly been limited to theoretical treatments and not solving real problems. Further Reading I am currently on the hunt for more to read on this topic, as I am new to it. I have found some very interesting stuff from the folks at the Santa Fe Institute, specifically at the Collective Computation Group .","title":"Biology's complexity problem"},{"location":"posts/2020/bio-future/#highlights","text":"In highly complex systems such as computers or living things, * complexity exists at many scales, and it is nontrivial to figure out appropriate coarse-grained or abstracted descriptions of the system's operation * high levels of variability may exist on multiple scales, but certain features are conserved . Knowing the difference between relevant and superfluous variability for a given question allows dimensionality reduction and is essential for reducing the difficulty of understanding these systems * \"understanding\" can be defined in many ways, and the level of abstraction allowed will depend on the specific questions or problems at hand. Knowing the most effective \"levers\" to manipulate the system at multiple scales could be a general goal of efforts to solve problems in a given system.","title":"Highlights"},{"location":"posts/2020/bio-future/#biologys-complexity-problem","text":"Biological research is still in its infancy in terms of performing precisely controlled experiments, as only around 40 years ago we did not even have the tools to perturb the informational language of cells - recombinant DNA and genetic engineering. Now that these exist and are rapidly improving, biology has entered an exciting age of discovery and advancement. However, we still largely lack the tools to precisely manipulate small numbers of cells in complex ways, or to make highly precise measurements on many parameters simultaneously. In the first case, desired or unexpected outcomes may have been missed because the \"right\" perturbation may not be simple. In the second case, an interesting outcome could have been missed because the \"right\" parameters (possibly thousands) were not measured. In my opinion, biology needs to be able to induce highly complex perturbations and measure simple outcomes, or induce simple perturbations and measure highly complex outcomes. Right now, we are mostly inducing simple perturbations and measuring simple outcomes.","title":"Biology's complexity problem"},{"location":"posts/2020/bio-future/#aliens-discover-a-dell","text":"I think the following thought experiment demonstrates well the difficulties of studying biological complexity. There is no reason to suspect our brains evolved to introspect into their own inner workings as we have been attempting. To model a similar scenario, imagine that a race of aliens visits earth and discovers our personal computers. Though they're intelligent, they have much different physiological and sensory parameters than us, and their equivalent of a computer is nothing like ours. Perhaps they interact with it not through touch and sight, but through electromagnetic radiation they emit, or precise mechanical vibrations. They don't necessarily use written language and certainly don't understand ours. Anyway, they decide to try to perform experiments on some earthly computers they found to try and understand what they do and how they work. How would this turn out? Because they have a very different way of interacting with their environment, their intuition will be mostly wrong. They will probably try first to interact with our computer as they do with theirs. This could lead to breaking a bunch of test objects, as their mechanical force or radiation or whatever is not compatible with the construction of our computers. Perhaps eventually they find the right scale of force to apply without breaking stuff. Assuming they have a properly-connected keyboard and mouse, maybe they try pressing many combinations of buttons at different times. Eventually they are lucky enough to figure out that pushing one particular button - the power button - results in a dramatic response: the display starts emitting light in complicated, time-dependent patterns! This would be an alien Nobel-worthy achievement. They now can seek to understand how the inputs correspond with the light patterns on the display, and how this is all coordinated under the hood. However, the space of possible inputs is gigantic - will they try every combination of the many keyboard buttons across many time points before or after the pressing of the power button? That will take a tremendous amount of time. We humans know that almost all of their attempts will do nothing meaningful. However, suppose they eventually somehow log in to the desktop. What a breakthrough! But they will find much a higher level of complexity in the on-screen patterns, and moreover that the patterns seem to change stochastically. What if the computer needs to update? What if Chrome opens on startup? It would be very easy to erroneously attribute these 'random' changes to the detailed sequence of perturbations (i.e. button presses, etc) that the experimenters are using, because they do not know the underlying principles. If the experimenters understand electricity, maybe they can grasp from their dissection efforts that the innards of the computer conduct electric current in highly complex and precise ways. They could then take a more bottom-up approach by trying to figure out which channels and components are important for the apparent function of the computer. This too is an enormous problem space - there are literally billions of transistors in a modern CPU alone. Of course, what readout they use of 'function' is very important: if all they have is whether the display lights up when the power button is pressed, they will not gain much information about how the component in question actually contributes to the computer's function. Let's say the experimenters had a few different types of computers - Windows and Mac, laptops and desktops, maybe even smartphones. How will they know the commonalities? From zero knowledge, different types of computers look very distinct. It should become clear that they all have a display, and some keyboard interface to input information: these are conserved features which can hint at their importance. However, let's also say they have two different models of Windows PCs - a Dell laptop and a Lenovo laptop, for example. While they look mostly similar, the layout and dimensions of the keyboards are noticeably different, there are some indicator lights and buttons that are different between them, and their on-screen light patterns differ somewhat. How will the experimenters determine if these differences are important or not? This illustrates the challenge of identifying significant variability . The significance of a given type of variability always depends on one's purpose - for certain questions, it matters a lot whether you have a Mac or PC, but for other more \"conserved\" features it doesn't. If you want to browse the web, the hardware of the computer doesn't really matter; if you want to perform complex climate modeling, it matters a great deal.","title":"Aliens discover a Dell"},{"location":"posts/2020/bio-future/#discussion","text":"The above few examples illustrate some basic absurdities and problems encountered when studying a complex multiscale system even in relatively productive lines of inquiry - knowing that the computer works by orchestrating the flow of electric current, and figuring out how to power it on and use the keyboard for inputs are correct tracks. Similarly, we have made great progress with identifying conserved low-level elements of biology in the form of the genetic code, the central dogma, and molecular evolution. Most people with a decent grasp on coding and basic hardware would probably say they \"know how a computer works\", yet they do not know or understand the complete circuit diagram of any part of the machine. But these tools are the levers which most effectively manipulate computers at most scales in a general way. A complete reductionist understanding of the atomistic parts isn't necessary because we have added layers of abstraction on top of it that allow us to effectively manipulate the computer's function without directly dealing with this complexity. The closest one can get to the circuits on most computers is assembly language, which still doesn't deal directly with transistors and logic gates. In a system as complex as a modern computer, it would simply require too much information to deal with these details. Similarly, changes to these low-level details tend not to affect the higher-level behavior in very noticeable ways because they are designed to smooth over and adapt to many different low-level configurations. Changes at all levels seem to become more important when the system is pushed to a limit, though: when running extremely intensive computations, the CPU architecture, assembly language, cache sizes and other low-level features start to matter a lot. Perhaps similarly, the function of many genes or other biological components becomes most apparent in stressed or diseased organisms. Layers of abstraction also allow us to disregard certain levels of variability. While the circuit diagrams of two different brands of motherboards or CPUs may differ wildly, only the bottom-most levels of the system interact with them, and they present a consistent coarse-grained picture to the upper levels so that the overall operation of the system is mostly unchanged. Abstraction and coarse-graining allow underspecification of low-level details, like how macroscopic thermodynamic observables describe a many-component system without specifying the position and momentum of each component. I think a major challenge of contemporary biology is determining which variability is important for a given question and which can be coarse-grained out. With increasing ability to generate huge, high-dimensional datasets, we always observe variability at many levels. Without further insights into which quantities or characteristics are conserved or important for given questions, we are stuck in a data-rich, understanding-poor regime. We are also prone to erroneously attribute causal power to coincidental correlations. Think of the aliens recording the shape and size of every key on a Mac versus a PC and positing that these are causally relevant to the differences between the two. Fixing most problems in a computer also doesn't require detailed knowledge of the low-level details. The command line and GUI present the levels for manipulating things at many levels. Hardware problems are usually handled by modularity - instead of tracking down the exact circuit that is causing a GPU failure, the entire unit can be removed and replaced without understanding any of the details. Medicine does a similar thing with organ transplants - we don't know how to grow a new heart yet or exactly understand how it works at every level, but we have identified empirical \"boundaries\" that allow it and other organs to be treated (imperfectly) as modules. Similarly in software, portions of the RAM content of a modern computer can be examined in relative isolation as the operating system logically allocates them to processes. I wonder if understanding \"modules\" at many levels and the levers that manipulate them will be a fruitful approach for human diseases. Traditional pharmaceutical approaches tend to target low-level elements such as specific proteins that are thought to be compromised in a disease. These approaches lie on the assumption that molecular targets are an appropriate module and lever for modulating the system's behavior. They work best when this assumption is most true - digestive enzyme deficienies and cystic fibrosis come to mind. But they tend to fail or have mixed results with conditions that are multiscale or involve high order interactions, like mental health conditions. Imagine trying to fix a problem in Microsoft Word by inactivating a certain capacitor on the motherboard that interacts with Word. It might sort of fix (at least the symptoms) of the problem, but many other programs depend on that component, and Word is affected by many subsystems of the computer beyond that component. High-level interventions are showing promise for certain extremely complex conditions, especially neurodegenerative and mental health conditions for which typical reductionist approaches fall short. The major challenge is that unlike computers, biology was not designed by humans and had no reason to designate scales, modules, and abstraction levels according to any scheme that is intuitive or sensical to our brains. It verges on an epistemological question whether we can ever fully grasp these partitions, if they exist. It seems to me that using mathematical principles to remove our flawed intuition and biases is important (however, mathematics may be a construct reflecting the structure of the human mind as well; perhaps a future post on this).","title":"Discussion"},{"location":"posts/2020/bio-future/#conclusion-where-from-here","text":"At the beginning I mentioned the scale of perturbations and measurements we're able to make. It would seem one way the aliens could proceed is by high-dimensional perturbations followed by simple readouts, such as whether the computer turns on. Another way is simple perturbations followed by high-dimensional readouts, such as contents of the display, voltage recordings on various components, etc. Both of these options are not great. They both involve 'brute-force' explorations of enormous spaces of possible perturbations or measurements. Because we designed computers to make sense, we know exactly which perturbations and measurements one should make. But with our own biology, we have only beginnings of ideas of what scales and levers are most relevant to perturb and measure. I tend to prefer the one perturbation-many measurements regime, which is what I work on in my PhD and what a lot of modern biology focuses on, but I'm not sure it's objectively superior. Both spaces, of perturbations and measurements, are basically infinite a priori , but the space of practical measurements is limited/defined by the current experimental methods. This makes it seem more tractable to try to understand effects of certain simple perturbations at many scales as measurement methods improve. Plus it seems easier to associate cause with a simple perturbation than with many, where further dissection would be required to identify necessary and accessory elements. Both perturbation and measurement techniques encounter difficulties applying across scales, for example to in vitro cultured cells then to whole organisms. Measuring low-level parameters such as individual genes in individual cells is possible in vitro and in small organismal systems such as parts of mouse organs or embryos; but it becomes virtually intractable at the scale of whole mouse (let alone human) organs, since these datasets scale with the volume of the system. The neuroscience subfield of connectomics encounters dramatically these scaling problems. This field attempts to produce a connection map of all the synapses of an organism's nervous system. So far, only the ~6000 connections in the millimeter-scale nematode Caenorhabditis elegans have been fully mapped. Heroic efforts are currently underway to map the millions of connections in the fruitfly, producing an enormous amount of data. Scaling up to mouse and eventually human seems impractical at the current rate of experiments, and the capacity of data storage and analysis: the human brain has on the order of $10^{15}$ synapses. Furthermore, this \"circuit diagram\" approach would seem to encounter a big problem of inter-individual variability. C. elegans have a prototyped nervous system that forms about the same connections each time - each of its few hundred neurons has its own name and neighbors. More complex organisms will not provide such a luxury. Once the incredible effort to map the fruitfly or rodent connectome is complete, the question will remain: what is the significance of each connection? How prototyped are they? The C. elegans connectome paved the way for much smarter questioning and is an invaluable reference, but it by no means \"solved\" the animal's neuroscience. Similarly, knowing the incredibly complex circuit diagram of my Dell laptop will not mean the aliens have finished their task and understand human-made computers. Moreover, this knowledge will hardly transfer to a different brand and architecture of computer, even though we know that these share many abstract similarities. Some knowledge of the conserved features that provide for abstraction and coarse-graining is necessary. Overall, we probably need to take measurements at multiple scales, and consider how layers of abstraction might be built in to biology for robustness and efficiency. What \"interfaces\" do low-level components provide for larger-scale manipulation? What assumptions do higher-level modules make about lower-level ones? I think we have enough knowledge to make some educated first guesses about some of this. I'm not sure if we have the measurement tools yet to directly query them. That is perhaps why this type of complex systems thinking applied to biology has mostly been limited to theoretical treatments and not solving real problems.","title":"Conclusion: where from here?"},{"location":"posts/2020/bio-future/#further-reading","text":"I am currently on the hunt for more to read on this topic, as I am new to it. I have found some very interesting stuff from the folks at the Santa Fe Institute, specifically at the Collective Computation Group .","title":"Further Reading"},{"location":"posts/2020/prestige-tone/","text":"When I was growing up and until recently, I was subject to the popular delusion that institutions such as Ivy League and other vaunted colleges and universities were \"the best\" in the world. What \"the best\" meant was not so clear in anything other than a vague feeling of superiority and the image of myriad articles on discoveries or inventions or popular media appearances of these institutions. When I got into Caltech, I was ecstatic that I was given a place at one of these \"top\" places. I thought there must be a superior quality of people at any such place, and going to one was a validation of one's ambitions and ideas - and a guarantee of success. After a year or so at the school and the inevitable dulling and tempering of one's expectations that occurs when the reality of a long-imagined possibility is finally experienced, I realized most of this wasn't true, and how hard it is to define what this idea of being the \"best\" meant. Does it mean there is no one at any \"lesser\" place doing work better than anyone at Caltech? Does it mean the average quality of work is better than at other places? Or that researchers have more money and resources? How do you judge the quality of work? The overall goal of this post is to ask what, if any, advantage prestigious institutions actually confer in terms of the experience of working at one. Tone setting journals It was shown to me that the popular way to judge work in the physical sciences was to look at which journal it was published in. If it was in Nature , Science , or Cell , it was \"big\" and probably high quality. Everyone was working to get their work accepted into these \"CNS\" journals. So does a top research university earn its place by publishing a high proportion of works in CNS? It quickly produced cognitive dissonance in me because I often found that articles from my field in these journals, and their still prestigious subjournals, were hard to read and repetitive - often small updates or advances over a previous method, executed on a large and clearly exorbitantly expensive scale. And others in my lab often judged them harshly, yet we still strived to publish in these journals. It eventually became clear that these journals are not necessarily better than any others, but that they are famous and read by a lot of people. People across the scientific community look to these journals as tone setters in their respective fields. Scientific research is vast and proceeds at a pace incomprehensible to any one person. Since no one is able to track and read even 10% of the articles that come out in any relatively active field, we develop heuristics to act as proxies for the scientific record. We all try to maintain a conception of the state of the field of our interest. I believe that this is formed and updated by considering only a small number of \"landmark\" institutions. We consider articles in three to ten journals perhaps, to get an approximate understanding of the going concerns of our field and the big developments that might be likely to influence one's perspective on one's own research path. This does not mean that important results are only submitted to this short list of journals. It means that these journals set the tone for the field. It means enough people use them as proxies for the state of their field that a certain consensus forms. This feeds back into the perceived prestige and quality of these journals: aware that they are used as touchstones by a large portion of the community, we want our results to appear there so that they are read widely and considered into others' conception of the state of the field. The quality and importance of research is a complicated thing to measure, and lasting importance isn't usually clear until much later. But the desire to be influential makese it more likely that high quality and important research is submitted to these touchstone journals. And, crucially, the state of the field judged from these journals influences our decisions about what projects to pursue ,as well as funding agencies' ideas of what worthwhile projects are. This adds inertia to the set of questions and approaches found in this highly limited slice of the scientific research world. Tone setting universities I think the tone setting effect extends much further than just academic journals. I think an undeniable defining characteristic of high-ranking universities is that they set the tone for a wide variety of areas. Obviously, this includes research. People inside science and outside (importantly, the media) look at what is being done at prestigious institutions as a touchstone for the state of a field. How often do we hear or read \"Harvard economist says...\" or \"Stanford engineers develop...\" in headlines and media messaging? How often do we see faculty from such schools on the news? The more we see and hear of these institutions, the more reinforced is the notion that they are somehow \"superior\" to other places. It's kind of like the addage that if you repeat a lie enough times and loudly enough, it becomes true. In reality, on the ensemble level faculty at these places have just been lucky to work at an institution used as a touchstone by the media and general culture. Within science, we also look to prestigious institutions first for finding the state of a field, and we pre-judge a scientific paper to some degree by looking at where its authors work. So before we even read an abstract, we are already biased by the journal and affiliations of the authors. We think that results are more likely to be reliable and high quality if they are published by authors at a well-known institution. It is no coincidence that prestigious universities generally have the highest research output of all universities. We should not confuse this with quality. These places might have the same distribution of output quality (by any metric) as any other school, but their higher output means they are likely to produce more high-quality results. Conclusion The most important idea is this: the world is incredibly complicated, and there are so many move parts of different scales that it seems an impossible task to accurately understand the state of anything at any one time. \"Quality\" - of scientific research, of industrial or technological development, or of societies and governments - has no one accepted metric. Different considerations emerge as salient as one looks at an instutition at different scales, from individual people to global. In the face of this complexity, we develop (with no small role for chance) heuristics for understanding the world. One way to do this is to pick a small subset of institutions as touchstones. These institutions set the tone for a certain domain by becoming important data points in our individual approximate models of the domain. The importance we assign to these touchstones feeds back into increased attention, resources and influence. People want to be involved with setting the tone for a field. Investors believe their money well spent if it goes toward projects judged important by many. Investments give these institutions the capacity to pursue large-scale, high-impact projects. These projects strongly influence the state of a field, further reinforcing the institutions' status. None of this necessarily implies the work done by prestigious institutions is better than anything else. But resources and the widespread attraction to touchstone institutions may increase the probability that their work may end up being actually good and actually useful in the future.","title":"Prestigious institutions as tone setters"},{"location":"posts/2020/prestige-tone/#tone-setting-journals","text":"It was shown to me that the popular way to judge work in the physical sciences was to look at which journal it was published in. If it was in Nature , Science , or Cell , it was \"big\" and probably high quality. Everyone was working to get their work accepted into these \"CNS\" journals. So does a top research university earn its place by publishing a high proportion of works in CNS? It quickly produced cognitive dissonance in me because I often found that articles from my field in these journals, and their still prestigious subjournals, were hard to read and repetitive - often small updates or advances over a previous method, executed on a large and clearly exorbitantly expensive scale. And others in my lab often judged them harshly, yet we still strived to publish in these journals. It eventually became clear that these journals are not necessarily better than any others, but that they are famous and read by a lot of people. People across the scientific community look to these journals as tone setters in their respective fields. Scientific research is vast and proceeds at a pace incomprehensible to any one person. Since no one is able to track and read even 10% of the articles that come out in any relatively active field, we develop heuristics to act as proxies for the scientific record. We all try to maintain a conception of the state of the field of our interest. I believe that this is formed and updated by considering only a small number of \"landmark\" institutions. We consider articles in three to ten journals perhaps, to get an approximate understanding of the going concerns of our field and the big developments that might be likely to influence one's perspective on one's own research path. This does not mean that important results are only submitted to this short list of journals. It means that these journals set the tone for the field. It means enough people use them as proxies for the state of their field that a certain consensus forms. This feeds back into the perceived prestige and quality of these journals: aware that they are used as touchstones by a large portion of the community, we want our results to appear there so that they are read widely and considered into others' conception of the state of the field. The quality and importance of research is a complicated thing to measure, and lasting importance isn't usually clear until much later. But the desire to be influential makese it more likely that high quality and important research is submitted to these touchstone journals. And, crucially, the state of the field judged from these journals influences our decisions about what projects to pursue ,as well as funding agencies' ideas of what worthwhile projects are. This adds inertia to the set of questions and approaches found in this highly limited slice of the scientific research world.","title":"Tone setting journals"},{"location":"posts/2020/prestige-tone/#tone-setting-universities","text":"I think the tone setting effect extends much further than just academic journals. I think an undeniable defining characteristic of high-ranking universities is that they set the tone for a wide variety of areas. Obviously, this includes research. People inside science and outside (importantly, the media) look at what is being done at prestigious institutions as a touchstone for the state of a field. How often do we hear or read \"Harvard economist says...\" or \"Stanford engineers develop...\" in headlines and media messaging? How often do we see faculty from such schools on the news? The more we see and hear of these institutions, the more reinforced is the notion that they are somehow \"superior\" to other places. It's kind of like the addage that if you repeat a lie enough times and loudly enough, it becomes true. In reality, on the ensemble level faculty at these places have just been lucky to work at an institution used as a touchstone by the media and general culture. Within science, we also look to prestigious institutions first for finding the state of a field, and we pre-judge a scientific paper to some degree by looking at where its authors work. So before we even read an abstract, we are already biased by the journal and affiliations of the authors. We think that results are more likely to be reliable and high quality if they are published by authors at a well-known institution. It is no coincidence that prestigious universities generally have the highest research output of all universities. We should not confuse this with quality. These places might have the same distribution of output quality (by any metric) as any other school, but their higher output means they are likely to produce more high-quality results.","title":"Tone setting universities"},{"location":"posts/2020/prestige-tone/#conclusion","text":"The most important idea is this: the world is incredibly complicated, and there are so many move parts of different scales that it seems an impossible task to accurately understand the state of anything at any one time. \"Quality\" - of scientific research, of industrial or technological development, or of societies and governments - has no one accepted metric. Different considerations emerge as salient as one looks at an instutition at different scales, from individual people to global. In the face of this complexity, we develop (with no small role for chance) heuristics for understanding the world. One way to do this is to pick a small subset of institutions as touchstones. These institutions set the tone for a certain domain by becoming important data points in our individual approximate models of the domain. The importance we assign to these touchstones feeds back into increased attention, resources and influence. People want to be involved with setting the tone for a field. Investors believe their money well spent if it goes toward projects judged important by many. Investments give these institutions the capacity to pursue large-scale, high-impact projects. These projects strongly influence the state of a field, further reinforcing the institutions' status. None of this necessarily implies the work done by prestigious institutions is better than anything else. But resources and the widespread attraction to touchstone institutions may increase the probability that their work may end up being actually good and actually useful in the future.","title":"Conclusion"},{"location":"posts/2021/tacit-dimension/","text":"The Tacit Dimension by Michael Polanyi I just finished reading Michael Polanyi's little book of lectures The Tacit Dimension . I found it quite interesting. The foreword classified it as something like \"deep insights presented one after the other\" - this is definitely accurate. In the first part, as the title suggests, he argues that all human knowledge is \"tacit\" in the same sense that we can pick the face of someone we know out of a large group without being able to describe how we can do so. I think it's something like dimensionality reduction in machine learning and statistics: our physiological and phenomenological processes are extremely complicated, and apparently we assign certain combinations of them to lower-dimensional objects of which we are aware. He mentions Gestalt psychology several times as holding some similar positions to his view. There is a general argument that wholes are more than the sum of their parts. He says all knowledge proceeds from internal \"proximal\" structures, which are tacit, to \"distal\" objects of which we are explicitly aware. We can only know the internal terms by their bearing on the external object. To me, this begs the question of whether the distal object can be repurposed as the proximal object for another round of tacit knowledge - a self-similar hierarchy. He does discuss the hierarchical nature of literature starting from alphabets, to words, to grammar, to prose. The rules of each \"lower\" level constrain the possibilities of \"higher\" levels, but they do not suggest or necessarily resemble the rules governing the higher levels. For example, rules of phonetics and spelling that determine words are pretty much unrelated to rules of grammar that make coherent sentences; and these rules in turn don't resemble at all the complicated principles of what makes good literature. He later says the higher levels impose on the lower levels boundary conditions - constraints that are not specifiable in the rules of the lower levels. I see how this applies to his later sections, but I'm not sure about what boundary conditions would mean in the case of literature. Science In the second lecture, he moves to a much broader project: applying this hierarchical structure to physical reality itself. This is a rejection of the logical positivist or reduction approach, especially in biology: the idea that ultimate understanding of life would mean reducing all organisms to physicochemical processes. In keeping with the whole>parts idea, he does not believe the characteristics of life are explainable by the characteristics of atoms and molecules, because life constitutes a higher level of reality. As above, it depends on the working of the physicochemical laws, but it imposes new constraints and new principles to which the laws of physics are not beholden. Emergence in biology An interesting aspect that Polanyi highlights is the introduction of a sense of purpose or drive with the emergence of life-like systems. One cannot speak of an inanimate natural system \"succeeding\" or \"failing\" - but with organisms, as with manmade machines, there is a clear notion of success and failure. These things seem to serve a purpose. Organisms succeed when they carry out the functions necessary to survive and reproduce, they fail when they die. With increasing biological or machine complexity come more failure points, more sub-purposes that comprise the overall purpose of the object. Think of all the component parts of a human body - each organelle in each cell in each tissue type in each organ and so on - all seemingly fulfilling their purpose so that the next higher level of organization can function. Every level of the system is characterized by its purpose, by the role it needs to fulfill that contributes to the overall human. Of course, we aren't so clear on the purpose of our humanity - our cells and tissues and organs might be quite angry to hear that all their hard work goes to waste creating an organism that is quite confused about its place in the world. The attempts we make to grasp our purpose - morality, theology, and so on - could be attempts to seek a higher organizational principle in the same vein as the hierarchy of purposeful elements that comprises our bodies. This fits in neatly with the theory because Polanyi argues that elements at a lower level are not aware of the principles guiding the higher level, hence the surprising aspect of emergence. This could certainly be noted in the difficulty of predicting the behavior of large-scale manmade systems like the world economy. What would boundary conditions mean for our humanity itself, though? My first thought is the parameters of our lives - our birth and death, and maybe our mental and physical limits. These appear emergent, as there isn't an obvious mechanistic correlate in the lower-level workings of our physiology that completely explains these limits; however, modern biology (in the 54 years since the book was published) has made quite a dent in finding biochemical or at least cellular explanations for ageing, embryonic developmenti, and mental capacity. Still, it is not that clear why there are many mammals with relatively similar organs but vastly different lifespans and intelligence levels. One nonliving system that appears to demonstrate some \"purpose\" is the weather. I'm thinking of the movement of clouds - from large volumes of air, water and particulates form discrete, extremely intricate structures which seem to move and interact on their own accord and cause complicated effects that one would never assign to \"just air and water\". Society Polanyi goes on to propose a human society based on the practice of the scientific or artistic community - one based on each person being knowledgeable about a tiny domain, and auditing/reviewing the conduct of the relatively few people whose domains overlap with their own. The assumption is that these domains together fully cover the space of science or art or society, so a sort of distributed moderation arises. While the picture he paints of the practice of science is very accurate and perceptive in my opinio, I won't comment much on this lofty proposal here. Final thoughts For me, the most salient points of this book are the ones about knowledge and biology/complexity. This book certainly spurs the view that complex systems must be understood on their own terms, and not reduced too aggressively to their component parts. The piece that is most exciting to me is the implication that theories for biology on its own terms are needed, potentially completely new types of theories and formalisms to describe the emergent rules that are apart from the physical and chemical underpinnings of all life.","title":"Thoughts on \"The Tacit Dimension\" by Michael Polanyi"},{"location":"posts/2021/tacit-dimension/#the-tacit-dimension-by-michael-polanyi","text":"I just finished reading Michael Polanyi's little book of lectures The Tacit Dimension . I found it quite interesting. The foreword classified it as something like \"deep insights presented one after the other\" - this is definitely accurate. In the first part, as the title suggests, he argues that all human knowledge is \"tacit\" in the same sense that we can pick the face of someone we know out of a large group without being able to describe how we can do so. I think it's something like dimensionality reduction in machine learning and statistics: our physiological and phenomenological processes are extremely complicated, and apparently we assign certain combinations of them to lower-dimensional objects of which we are aware. He mentions Gestalt psychology several times as holding some similar positions to his view. There is a general argument that wholes are more than the sum of their parts. He says all knowledge proceeds from internal \"proximal\" structures, which are tacit, to \"distal\" objects of which we are explicitly aware. We can only know the internal terms by their bearing on the external object. To me, this begs the question of whether the distal object can be repurposed as the proximal object for another round of tacit knowledge - a self-similar hierarchy. He does discuss the hierarchical nature of literature starting from alphabets, to words, to grammar, to prose. The rules of each \"lower\" level constrain the possibilities of \"higher\" levels, but they do not suggest or necessarily resemble the rules governing the higher levels. For example, rules of phonetics and spelling that determine words are pretty much unrelated to rules of grammar that make coherent sentences; and these rules in turn don't resemble at all the complicated principles of what makes good literature. He later says the higher levels impose on the lower levels boundary conditions - constraints that are not specifiable in the rules of the lower levels. I see how this applies to his later sections, but I'm not sure about what boundary conditions would mean in the case of literature.","title":"The Tacit Dimension by Michael Polanyi"},{"location":"posts/2021/tacit-dimension/#science","text":"In the second lecture, he moves to a much broader project: applying this hierarchical structure to physical reality itself. This is a rejection of the logical positivist or reduction approach, especially in biology: the idea that ultimate understanding of life would mean reducing all organisms to physicochemical processes. In keeping with the whole>parts idea, he does not believe the characteristics of life are explainable by the characteristics of atoms and molecules, because life constitutes a higher level of reality. As above, it depends on the working of the physicochemical laws, but it imposes new constraints and new principles to which the laws of physics are not beholden.","title":"Science"},{"location":"posts/2021/tacit-dimension/#emergence-in-biology","text":"An interesting aspect that Polanyi highlights is the introduction of a sense of purpose or drive with the emergence of life-like systems. One cannot speak of an inanimate natural system \"succeeding\" or \"failing\" - but with organisms, as with manmade machines, there is a clear notion of success and failure. These things seem to serve a purpose. Organisms succeed when they carry out the functions necessary to survive and reproduce, they fail when they die. With increasing biological or machine complexity come more failure points, more sub-purposes that comprise the overall purpose of the object. Think of all the component parts of a human body - each organelle in each cell in each tissue type in each organ and so on - all seemingly fulfilling their purpose so that the next higher level of organization can function. Every level of the system is characterized by its purpose, by the role it needs to fulfill that contributes to the overall human. Of course, we aren't so clear on the purpose of our humanity - our cells and tissues and organs might be quite angry to hear that all their hard work goes to waste creating an organism that is quite confused about its place in the world. The attempts we make to grasp our purpose - morality, theology, and so on - could be attempts to seek a higher organizational principle in the same vein as the hierarchy of purposeful elements that comprises our bodies. This fits in neatly with the theory because Polanyi argues that elements at a lower level are not aware of the principles guiding the higher level, hence the surprising aspect of emergence. This could certainly be noted in the difficulty of predicting the behavior of large-scale manmade systems like the world economy. What would boundary conditions mean for our humanity itself, though? My first thought is the parameters of our lives - our birth and death, and maybe our mental and physical limits. These appear emergent, as there isn't an obvious mechanistic correlate in the lower-level workings of our physiology that completely explains these limits; however, modern biology (in the 54 years since the book was published) has made quite a dent in finding biochemical or at least cellular explanations for ageing, embryonic developmenti, and mental capacity. Still, it is not that clear why there are many mammals with relatively similar organs but vastly different lifespans and intelligence levels. One nonliving system that appears to demonstrate some \"purpose\" is the weather. I'm thinking of the movement of clouds - from large volumes of air, water and particulates form discrete, extremely intricate structures which seem to move and interact on their own accord and cause complicated effects that one would never assign to \"just air and water\".","title":"Emergence in biology"},{"location":"posts/2021/tacit-dimension/#society","text":"Polanyi goes on to propose a human society based on the practice of the scientific or artistic community - one based on each person being knowledgeable about a tiny domain, and auditing/reviewing the conduct of the relatively few people whose domains overlap with their own. The assumption is that these domains together fully cover the space of science or art or society, so a sort of distributed moderation arises. While the picture he paints of the practice of science is very accurate and perceptive in my opinio, I won't comment much on this lofty proposal here.","title":"Society"},{"location":"posts/2021/tacit-dimension/#final-thoughts","text":"For me, the most salient points of this book are the ones about knowledge and biology/complexity. This book certainly spurs the view that complex systems must be understood on their own terms, and not reduced too aggressively to their component parts. The piece that is most exciting to me is the implication that theories for biology on its own terms are needed, potentially completely new types of theories and formalisms to describe the emergent rules that are apart from the physical and chemical underpinnings of all life.","title":"Final thoughts"},{"location":"posts/2021/two-times/","text":"Time Barring relativistic effects, physical time passes at a constant rate (though this is a circular statement, as \"constant rate\" is defined by time itself). We can use a mechanical or digital device to gauge the passage of time as a uniform change in some mechanism in the device. However, we don't experience it like this. We ubiquitously experience an acceleration of subjective time as we age. We commonly complain that a vacation \"went too fast\", or an unpleasent week \"took forever\". We have certain ideas of what a different time intervals \"feel like\", and when a particular instance of an interval doesn't match up with our conception, we feel a bit of dissonance. Where do these ideas come from? How often are they updated? How precise are they? Time as change As an initial guess, I would think the subjective experience of time has to do with quantifying change over a certain interval. Or perhaps \"surprise\" is a better word, because it supports the following. As children, most of the events we experience are newer to us than they are as adults. Maybe time seems to go slower in childhood because we are surprised and interested by more things. We have less concept of past. Later on, a week in which one has to deal with many unexpected and/or difficult things usually seems longer than one in which nothing \"exciting\" happens, a completely routine week. Present versus past time I think there is a distinction between the rate of time as we experience an event, and the apparent length of a past event. There is an asymmetry. For example, the 25 years of my life leading up to now don't seem so long, as here I am in the present, having lived them. But the prospect of waiting another 25 years for something seems inconceivably long - a scale I have no reference for. This is because of uncertainty. The present is much more rich than the past. The past is a fragmented, simplified concept which is \"settled\" in the sense that we think of it as unchangeable (I am resisting the urge to say \"in the past\"...) and somehow \"solid\". Reality vs. desired There may also be an element of a desired time experience. Usually, we want unpleasant experiences to pass quickly and pleasant ones to pass slowly. In fact, we want unpleasant experiences to pass immediately, and pleasant ones to last forever. So reality always seems, respectively, too slow and too fast. This does suggest an objective statement: change happens; we cannot stop or infinitely accelerate subjective time, though we are forever stuck firmly in the present moment. Time and temperature Similarly, we experience subjective temperature much differently than physical temperature. Though two people's internal body temperatures are usually within a degree or two, their subjective experiences can easily be searing heat versus frigid cold. This can be the case in the same ambient temperature as well. The subjective experience seems to depend on the difference between the ambient and body temperatures, preferences or mechanisms intrinsic to the individual, and one's current physical and emotional state (e.g., exercising or being angry or embarrassed cause a feeling of heat). The concept of the past and future But temperature is much easier to grasp and analyze than time. Time is a funny thing to conceive of, perhaps simply because it is one of the dimensions of the space through which we move. Trying to describe time is like trying to describe the experience of \"forwardness\" or \"leftness\". And yet we strongly want to analyze time, to capture, quantify and understand its passage. Why does it feel like we're moving through it? Because we have memory, and the ability to conceive of situations either in the past or future. Because we have the concepts of past and future. The past and future are concepts. I don't know if they're real or not. When we think of the past or future, we are always bound to the present. An argument for the reality of the past is the fact that people who don't know each other will nonetheless agree on certain aspects of their concept of the past. This is History: a partially shared agreement on concepts under the heading of \"the past\".","title":"Physical time versus subjective time"},{"location":"posts/2021/two-times/#time","text":"Barring relativistic effects, physical time passes at a constant rate (though this is a circular statement, as \"constant rate\" is defined by time itself). We can use a mechanical or digital device to gauge the passage of time as a uniform change in some mechanism in the device. However, we don't experience it like this. We ubiquitously experience an acceleration of subjective time as we age. We commonly complain that a vacation \"went too fast\", or an unpleasent week \"took forever\". We have certain ideas of what a different time intervals \"feel like\", and when a particular instance of an interval doesn't match up with our conception, we feel a bit of dissonance. Where do these ideas come from? How often are they updated? How precise are they?","title":"Time"},{"location":"posts/2021/two-times/#time-as-change","text":"As an initial guess, I would think the subjective experience of time has to do with quantifying change over a certain interval. Or perhaps \"surprise\" is a better word, because it supports the following. As children, most of the events we experience are newer to us than they are as adults. Maybe time seems to go slower in childhood because we are surprised and interested by more things. We have less concept of past. Later on, a week in which one has to deal with many unexpected and/or difficult things usually seems longer than one in which nothing \"exciting\" happens, a completely routine week.","title":"Time as change"},{"location":"posts/2021/two-times/#present-versus-past-time","text":"I think there is a distinction between the rate of time as we experience an event, and the apparent length of a past event. There is an asymmetry. For example, the 25 years of my life leading up to now don't seem so long, as here I am in the present, having lived them. But the prospect of waiting another 25 years for something seems inconceivably long - a scale I have no reference for. This is because of uncertainty. The present is much more rich than the past. The past is a fragmented, simplified concept which is \"settled\" in the sense that we think of it as unchangeable (I am resisting the urge to say \"in the past\"...) and somehow \"solid\".","title":"Present versus past time"},{"location":"posts/2021/two-times/#reality-vs-desired","text":"There may also be an element of a desired time experience. Usually, we want unpleasant experiences to pass quickly and pleasant ones to pass slowly. In fact, we want unpleasant experiences to pass immediately, and pleasant ones to last forever. So reality always seems, respectively, too slow and too fast. This does suggest an objective statement: change happens; we cannot stop or infinitely accelerate subjective time, though we are forever stuck firmly in the present moment.","title":"Reality vs. desired"},{"location":"posts/2021/two-times/#time-and-temperature","text":"Similarly, we experience subjective temperature much differently than physical temperature. Though two people's internal body temperatures are usually within a degree or two, their subjective experiences can easily be searing heat versus frigid cold. This can be the case in the same ambient temperature as well. The subjective experience seems to depend on the difference between the ambient and body temperatures, preferences or mechanisms intrinsic to the individual, and one's current physical and emotional state (e.g., exercising or being angry or embarrassed cause a feeling of heat).","title":"Time and temperature"},{"location":"posts/2021/two-times/#the-concept-of-the-past-and-future","text":"But temperature is much easier to grasp and analyze than time. Time is a funny thing to conceive of, perhaps simply because it is one of the dimensions of the space through which we move. Trying to describe time is like trying to describe the experience of \"forwardness\" or \"leftness\". And yet we strongly want to analyze time, to capture, quantify and understand its passage. Why does it feel like we're moving through it? Because we have memory, and the ability to conceive of situations either in the past or future. Because we have the concepts of past and future. The past and future are concepts. I don't know if they're real or not. When we think of the past or future, we are always bound to the present. An argument for the reality of the past is the fact that people who don't know each other will nonetheless agree on certain aspects of their concept of the past. This is History: a partially shared agreement on concepts under the heading of \"the past\".","title":"The concept of the past and future"}]}